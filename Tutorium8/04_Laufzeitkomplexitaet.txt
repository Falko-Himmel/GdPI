Wenn wir Algorithmen schreiben ist es nicht nur wichtig, dass diese nicht nur das richtige Ergebnis liefern, sondern auch schnell bzw. performant sind.
Um dabei diese performance messen zu kÃ¶nnen, kÃ¶nnen wir dies entweder theoretisch oder praktisch angehen.
Dabei haben beide Messmethoden jeweil Vor- und Nachteile.

Bei praktschen Messungen messen wir wirklich die tatsÃ¤chliche Zeit, die ein Algorithmus fÃ¼r die Abarbeitung einer Eingabe benÃ¶tigt. 
Dann kÃ¶nnen wir mit diesen Messungen (ggf. Ã¶fters messen und mitteln) unseren Algorithmus vergleichen.
Die Eingaben nennt man dabei Benchmarks, die eine Sammlung an Eingaben darstellt.

+ einfach in der umsetzung
- abhÃ¤ngig von Umgebung
- nicht immer gut vergleichbar



Die theoretische Laufzeitanalyse (i.d.R. das relevante fÃ¼r die Klausur) bestimmen wir die KomplexitÃ¤t, also das Laufzeitverhalten im asymptotischen Verlauf der EingabegrÃ¶ÃŸe.
Hier wird sich auf ZeitaufwÃ¤ndige Operationen konzentriert. Z.B. Addition wird hier als konstant betrachtet und flieÃŸt "nicht so wesentlich ein".
Dabei wird angenommen, dass: 

â€¢ Speicherreservierungen benÃ¶tigen keine wesentliche Zeit
â€¢ Programmflusssteuerung benÃ¶tigt keine wesentliche Zeit
â€¢ Elementarschritte werden gezÃ¤hlt: Zuweisungen (Z), Vergleiche (V), Additionen & Subtraktion (A/S) und Multiplikationen (M)
â€¢ Elementaroperationen benÃ¶tigen gleich viel Zeit (konstant)

Wie genau kÃ¶nnen wir jetzt die KomplexitÃ¤t tatsÃ¤chlich bestimmen von einem Algorithmus?
Wir haben die ZeitkomplexitÃ¤t T(n), die angibt wie viele Elementaroperationen es benÃ¶tigt ein Problem mit EingabegrÃ¶ÃŸe n zu lÃ¶sen.

Bsp.:
fak :: Int -> Int
fak 0 = 0 -- Konstante Zeit O(1)
fak n = n * fak (n-1) -- n * ist Konstante Zeit wegen Elementaroperationen + T(n-1)

Daraus erhalten wir die Rekurrenzgleichung mit T(n) = T* + T(n-1)

Da wir quasi nur die Annahmen gemacht haben, dass Elementarschritte und co konstante oder keine wesentliche Zeit brauchen, ist die tatsÃ¤chliche grÃ¶ÃŸe dieser Zeiteinheiten fÃ¼r die ZeitkomplexitÃ¤t unbekannt.
Dies ist dann HardwareabhÃ¤ngig, je nachdem, wo man diesen Algorithmus ausfÃ¼hrt ... .





=> jetzt die Erkenntnis: konstante Faktoren sind relativ "egal" und vernachlÃ¤ssigbar fÃ¼r groÃŸe Eingabedaten.
https://www.desmos.com/calculator/obdm27ccog

hier kann man sehen, dass im reingezoomten zustand noch ein erkennbarer unterschied zwischen dem Graph mit und ohne Faktor existiert, aber sobald man ein bisschen herauszoomed, werden die unterschiede zwischen der Laufzeit mit und ohne Faktor vernachlÃ¤ssigbar. (besonders wenn man vergleicht mit Funktionen anderer KomplexitÃ¤tsklassen)

=> auch kommt man zu der Erkenntnis, dass wenn man auf eine Funktion eine kleine "drauf addiert", diese im asymptotischen Verlauf keinen wesentlichen Unterschied macht.
https://www.desmos.com/calculator/ok8ycuugjl




aus diesen beiden Erkenntnissen kÃ¶nnen wir auf folgende Rechenregeln schlieÃŸen:


â€¢ ð‘“(ð‘›) = ð‘˜ âˆˆ ð‘‚(1) fÃ¼r eine Konstante ð‘˜
â€¢ ð‘“(ð‘›) âˆˆ ð‘‚(ð‘“(ð‘›))
â€¢ ð‘‚(ð‘ âˆ— ð‘“(ð‘›)) = ð‘‚(ð‘“(ð‘›)) fÃ¼r eine Konstante ð‘ > 0
â€¢ ð‘‚(ð‘“(ð‘›) + ð‘”(ð‘›)) = ð‘‚(ð‘šð‘Žð‘¥(ð‘“(ð‘›), ð‘”(ð‘›)))
â€¢ ð‘‚(ð‘“(ð‘›)) âˆ— ð‘‚(ð‘”(ð‘›)) = ð‘‚(ð‘“(ð‘›) âˆ— ð‘”(ð‘›))


Bei der O-Notation haben wir dabei die KomplexitÃ¤tsklassen:
konstant: ð‘‚(1)
logarithmisch: ð‘‚(ð‘™ð‘œð‘”(ð‘›))
linear: ð‘‚(ð‘›)
log linear: ð‘‚(ð‘™ð‘œð‘”(ð‘›) âˆ— ð‘›)
quadratisch: ð‘‚(ð‘›^2)
kubisch: ð‘‚(ð‘›^3)
polynomiell: ð‘‚(ð‘›^ð‘˜) mit Konstante ð‘˜ > 1
exponentiell: ð‘‚(2^ð‘›)

=> altklausurenaufgabe 2024_1 / Aufgabe 6
https://moodle.uni-ulm.de/pluginfile.php/1348606/mod_folder/content/0/2024_1_exam.pdf

LÃ¶sung: 
https://moodle.uni-ulm.de/pluginfile.php/1348606/mod_folder/content/0/2024_1_solution.pdf

